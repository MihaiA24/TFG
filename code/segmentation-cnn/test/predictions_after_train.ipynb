{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from datasetV2 import *\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from generate_results import *\n",
    "from model3 import jacard_coef,jacard_coef_loss,DiceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_4927/2412813334.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 10:17:29.456712: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-11 10:17:29.510927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-11 10:17:29.600839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-11 10:17:29.601201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-11 10:17:30.569321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-11 10:17:30.569628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-11 10:17:30.569819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-11 10:17:30.571342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 3372 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2022-06-11 10:17:30.578277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-11 10:17:30.578680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-11 10:17:30.578910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_available = tf.test.is_gpu_available()\n",
    "print(gpu_available)\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model = load_model('../Tensorflow_remote/model_parcial16V3_originalSize.h5',custom_objects={'jacard_coef_loss' : jacard_coef_loss,'jacard_coef' : jacard_coef})\n",
    "# model = load_model('../Tensorflow_remote/model_parcial16V3_originalSize.h5')\n",
    "\n",
    "model = load_model('trained_models/model_reduced_jac_jac_sizeX2_10ep_lr5E5_batch16.h5',custom_objects={'jacard_coef_loss' : jacard_coef_loss,'jacard_coef' : jacard_coef})\n",
    "# model = load_model('trained_models/model_reduced_jac_jac_sizeX2_1ep_lr1E4_batch8.h5',custom_objects={'jacard_coef' : jacard_coef})\n",
    "\n",
    "# model = load_model('trained_models/model_reduced_jac_jac_dice_sizeX2_20ep_lr5E5_var_batch16_new_jac.h5',custom_objects={'jacard_coef_loss' : jacard_coef_loss,'jacard_coef' : jacard_coef, 'DiceLoss' : DiceLoss})\n",
    "\n",
    "\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 144\n",
    "\n",
    "\n",
    "IMAGE_HEIGHT = 448\n",
    "IMAGE_WIDTH = 288\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_path =  'original csvs/test_dataset.csv' \n",
    "\n",
    "BATCH_SIZE = 16 \n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "test_images, test_masks = load_data(test_path)\n",
    "test_dataset = tf_dataset(test_images, test_masks,batch_size=BATCH_SIZE, buffer_size=BUFFER_SIZE,shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "original = []\n",
    "masks = []\n",
    "predicted = []\n",
    "ind = 0\n",
    "for elem in tqdm(test_dataset):\n",
    "    break\n",
    "    predictions = model.predict(elem[0])\n",
    "    for item in predictions:\n",
    "        predicted.append(item > 0.5)\n",
    "\n",
    "    for item in elem[0]: # image\n",
    "        original.append(item)\n",
    "\n",
    "    for item in elem[1]: # masks \n",
    "        masks.append(item)\n",
    "    ind = ind + 1\n",
    "    if ind == 32:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(original[0].shape)\n",
    "print(masks[0].shape)\n",
    "print(predicted[0].shape)\n",
    "print(np.unique(predicted[0]))\n",
    "\n",
    "print(len(original))\n",
    "print(len(masks))\n",
    "print(len(predicted))\n",
    "\n",
    "index = 147\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(original[index])\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(masks[index])\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(predicted[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# generate_results_reduced(dirname='test_results/model_reduced_jac_jac_dice_sizeX2_20ep_lr5E5_var_batch16_new_jac/',predictions= predicted,masks= masks,images_path= test_images)\n",
    "\n",
    "generate_results_reduced(dirname='test_results/model_reduced_jac_jac_dice_sizeX2_20ep_lr5E5_var_batch16_new_jac/',model= model,dataset= test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(cv2.imread(test_images[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(test_dataset,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predict = model.predict(test_dataset.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(predict[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "index = 308\n",
    "pred = predicted[index].astype(np.uint8)\n",
    "mask = masks[index].numpy().astype(np.uint8)\n",
    "\n",
    "mask = cv2.resize(mask,(720,1160),interpolation = cv2.INTER_NEAREST)\n",
    "pred = cv2.resize(pred,(720,1160),interpolation = cv2.INTER_NEAREST)\n",
    "\n",
    "img = cv2.imread(test_images[index])\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "res_pred = img\n",
    "# pred[pred == 1] = 255\n",
    "\n",
    "pred = pred.astype(np.uint8)\n",
    "\n",
    "contours,_ = cv2.findContours(pred, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "# rect = cv2.minAreaRect(contours[0])\n",
    "# print(rect)\n",
    "# box = cv2.boxPoints(rect)\n",
    "# box = box.astype(int)\n",
    "\n",
    "heigth = 70\n",
    "width = 180\n",
    "\n",
    "pt2 = np.float32([[0,0], [width,0], [0,heigth], [width,heigth]])\n",
    "\n",
    "approx = cv2.approxPolyDP(contours[0], 0.035 * cv2.arcLength(contours[0], False), True)\n",
    "\n",
    "box = np.squeeze(approx, axis=1)\n",
    "print(box)\n",
    "# if box[0][0] < box[2][0]:\n",
    "#     print('if')\n",
    "#     pt12 = np.float32([box[0],box[1],box[3],box[2]])\n",
    "#     box_aux = [box[0],box[1],box[3],box[2]]\n",
    "#     pt12 = np.float32([box[0],box[3],box[1],box[2]])\n",
    "\n",
    "# else:\n",
    "#     pt12 = np.float32([box[1],box[2],box[0],box[3]])\n",
    "#     box_aux = [box[1],box[2],box[0],box[3]]\n",
    "#     pt12 = np.float32([box[3],box[2],box[0],box[1]])\n",
    "#     pt12 = np.float32([box[1],box[0],box[2],box[3]])\n",
    "if box[0][0] < box[2][0]:\n",
    "    print('if')\n",
    "    pt12 = np.float32([box[0],box[3],box[1],box[2]])\n",
    "    if box[0][1] > box[2][1]:\n",
    "        pt12 = np.float32([box[3],box[2],box[0],box[1]])\n",
    "else: \n",
    "    print('else')\n",
    "    pt12 = np.float32([box[1],box[0],box[2],box[3]])\n",
    "    print(pt12)\n",
    "    if box[0][1] > box[2][1]:\n",
    "        pt12 = np.float32([box[2],box[1],box[3],box[0]])\n",
    "\n",
    "\n",
    "\n",
    "# pt12 = np.float32([box[0],box[3],box[1],box[2]])\n",
    "# pt12 = np.float32([box[3],box[2],box[0],box[1]])\n",
    "# pt12 = np.float32([box[2],box[1],box[3],box[0]])\n",
    "\n",
    "\n",
    "f = cv2.getPerspectiveTransform(pt12,pt2)\n",
    "f2 = cv2.warpPerspective(img,f,(width,heigth))\n",
    "\n",
    "\n",
    "ff = np.squeeze(approx, axis=1)\n",
    "# print(ff)\n",
    "# box = [i for in ]\n",
    "# print(box)\n",
    "# print(box_aux)\n",
    "box = ff\n",
    "# box = box_aux\n",
    "# print(approx[:4])\n",
    "    # draws boundary of contours.\n",
    "# for i in a\n",
    "# cv2.drawContours(pred, [approx[:4]], 0, (255), 5)\n",
    "for i in range(0,4):\n",
    "    cv2.circle(pred,(box[i][0], box[i][1]), 20, (i+1)*50, -1)\n",
    "\n",
    "plt.imshow(pred,cmap='gray')\n",
    "plt.imshow(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "batch_size = 8\n",
    "# index = 0\n",
    "\n",
    "dirname = 'test_results/model_fullNet/results/'\n",
    "listdir = os.listdir(dirname)\n",
    "for dir in listdir:\n",
    "    shutil.rmtree(dirname + dir)\n",
    "# reescale bin image\n",
    "os.mkdir(dirname + '!Todas')\n",
    "num = 0\n",
    "for index in tqdm(range(0,len(predicted))):\n",
    "    pred = predicted[index].astype(np.uint8)\n",
    "    mask = masks[index].numpy().astype(np.uint8)\n",
    "\n",
    "    mask = cv2.resize(mask,(720,1160),interpolation = cv2.INTER_NEAREST)\n",
    "    pred = cv2.resize(pred,(720,1160),interpolation = cv2.INTER_NEAREST)\n",
    "\n",
    "    img = cv2.imread(test_images[index])\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "\n",
    "    res_pred = img\n",
    "    pred[pred == 1] = 255\n",
    "    # print(np.unique(pred))\n",
    "\n",
    "\n",
    "\n",
    "    pred = pred.astype(np.uint8)\n",
    "\n",
    "    contours,_ = cv2.findContours(pred, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    # if len(contours)  == 0:\n",
    "    #     break\n",
    "    approx = cv2.approxPolyDP(contours[0], 0.05 * cv2.arcLength(contours[0], True), True)\n",
    "\n",
    "    # rect = cv2.minAreaRect(contours[0])\n",
    "    # box = cv2.boxPoints(rect)\n",
    "    # print(box)\n",
    "    # print(approx)\n",
    "    box = np.squeeze(approx, axis=1)\n",
    "    # cv2.drawContours(mask, [approx[:4]], 0, (255), 5)\n",
    "    # for i in range(0,4):\n",
    "    #     cv2.circle(mask,(box[i][0], box[i][1]), 25, (i+1)*50, -1)\n",
    "\n",
    "    heigth = 70\n",
    "    width = 180\n",
    "\n",
    "    pt2 = np.float32([[0,0], [width,0], [0,heigth], [width,heigth]])\n",
    "    # print(box)\n",
    "    if box[0][0] < box[2][0]:\n",
    "        pt12 = np.float32([box[0],box[3],box[1],box[2]])\n",
    "    else:\n",
    "        pt12 = np.float32([box[1],box[0],box[2],box[3]])\n",
    "        \n",
    "\n",
    "    f = cv2.getPerspectiveTransform(pt12,pt2)\n",
    "    f2 = cv2.warpPerspective(img,f,(width,heigth))\n",
    "\n",
    "\n",
    "    gray = cv2.cvtColor(f2,cv2.COLOR_BGR2GRAY)\n",
    "    # gray = f2[:,:,2]\n",
    "    # gray = cv2.bilateralFilter(gray,11,17,17)\n",
    "    canny = cv2.Canny(gray,40,200)\n",
    "    # lines = cv2.HoughLinesP(canny,1,np.pi/180,10, maxLineGap=150)\n",
    "\n",
    "    lines = cv2.HoughLinesP(canny,0.001,np.pi/180,3, minLineLength=40, maxLineGap=180)\n",
    "\n",
    "    # if lines is None:\n",
    "    #     break\n",
    "    \n",
    "\n",
    "    hough = np.zeros((heigth,width), np.uint8)\n",
    "    hough = canny.copy()\n",
    "\n",
    "\n",
    "    # for rho,theta in lines[0]:\n",
    "    #     a = np.cos(theta)\n",
    "    #     b = np.sin(theta)\n",
    "    #     x0 = a*rho\n",
    "    #     y0 = b*rho\n",
    "    #     x1 = int(x0 + 1000*(-b))\n",
    "    #     y1 = int(y0 + 1000*(a))\n",
    "    #     x2 = int(x0 - 1000*(-b))\n",
    "    #     y2 = int(y0 - 1000*(a))\n",
    "\n",
    "    #     cv2.line(hough,(x1,y1),(x2,y2),(255),1)\n",
    "    \n",
    "\n",
    "\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            x1,y1,x2,y2 = line[0]\n",
    "            cv2.line(hough,(x1,y1),(x2,y2),(255),1)\n",
    "\n",
    "    kernel = np.ones((int(heigth * 0.1)), dtype=np.uint8) * 255\n",
    "    # plt.imshow(gray)\n",
    "\n",
    "    opening = cv2.morphologyEx(hough, cv2.MORPH_OPEN, kernel)\n",
    "    # plt.imshow(opening)\n",
    "    boxop = getBoundingBoxFromMask(opening).astype(int)\n",
    "    \n",
    "    result = f2[boxop[0]:boxop[2],boxop[1]:boxop[3]]\n",
    "\n",
    "    if not result.any():\n",
    "        result = f2.copy()\n",
    "    resultSeg = result.copy()\n",
    "\n",
    "    he, wi,dm = resultSeg.shape\n",
    "    div = int(wi/7)\n",
    "\n",
    "    divs = [i * div + 5 for i in range(1,7)]\n",
    "    divs[0] = divs[0] - 5\n",
    "    divs[1] = divs[1] - 5\n",
    "    divs[-2] = divs[-2] - 5\n",
    "    divs[-1] = divs[-1] - 5\n",
    "\n",
    "    # print(divs,width,resultSeg.shape)\n",
    "\n",
    "\n",
    "    divImgList = [];\n",
    "\n",
    "    for i in divs:\n",
    "        # divImgList.append(resultSeg)\n",
    "        resultSeg[:,i-1:i+1] = 0\n",
    "        \n",
    "    # plt.imshow(canny)\n",
    "    dir = dirname + str(index) + '_0' +  str(index2) + '/'\n",
    "    # num = ((index + 1) * (index2 + 1))\n",
    "    num = num + 1\n",
    "    if num < 10:\n",
    "        dir = dirname + '0' + str( num ) + '/'\n",
    "    else:\n",
    "        dir = dirname + str( num ) + '/'\n",
    "\n",
    "    # dir = 'result/' + str(index) + '_0' +  str(index2) + '_'\n",
    "    os.mkdir(dir)\n",
    "\n",
    "    # ret, threshed_img = cv2.threshold(cv2.cvtColor(result,cv2.COLOR_BGR2GRAY),100, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY)\n",
    "    # threshed_img = cv2.adaptiveThreshold(cv2.cvtColor(result,cv2.COLOR_BGR2GRAY), 255, cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY, 11 , 10)\n",
    "    # ret, threshed_img = cv2.threshold(result[:,:,0],100, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY)\n",
    "\n",
    "    # cv2.imwrite(dir + '/'+ lista_dir[index][index2],cv2.cvtColor(img,cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(dir + '1_original.jpg',cv2.cvtColor(img,cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(dir + '3_prediction.jpg',pred)\n",
    "    cv2.imwrite(dir + '2_mask.jpg',mask * 255)\n",
    "    cv2.imwrite(dir + '4_perspectiva.jpg',cv2.cvtColor(f2,cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(dir + '5_canny.jpg',canny)\n",
    "    cv2.imwrite(dir + '5.6_morphology.jpg',opening)\n",
    "    if len(result) > 0:\n",
    "        cv2.imwrite(dir + '6_recortadaCanny.jpg',cv2.cvtColor(result,cv2.COLOR_RGB2BGR))\n",
    "        cv2.imwrite(dir + '7_recortadaCanny_CaracteresSegmentados.jpg',cv2.cvtColor(resultSeg,cv2.COLOR_RGB2BGR))\n",
    "        cv2.imwrite(dirname + '!Todas/' + str(num) + '_7_recortadaCanny_CaracteresSegmentados.jpg',cv2.cvtColor(resultSeg,cv2.COLOR_RGB2BGR))\n",
    "    # cv2.imwrite(dirname + '!Todas/' + str(index) + '_0' +  str(index2) + '_8_recortadaCanny_CaracteresSegmentados.jpg',cv2.cvtColor(threshed_img,cv2.COLOR_RGB2BGR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def getBoundingBoxFromMask(Y):\n",
    "    \"\"\"Convert mask Y to a bounding box, assumes 0 as background nonzero object\"\"\"\n",
    "    cols, rows = np.nonzero(Y)\n",
    "    if len(cols)==0: \n",
    "        return np.zeros(4, dtype=np.float32)\n",
    "    top_row = np.min(rows)\n",
    "    left_col = np.min(cols)\n",
    "    bottom_row = np.max(rows)\n",
    "    right_col = np.max(cols)\n",
    "    return np.array([left_col, top_row, right_col, bottom_row], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = '../ccpd_base/'\n",
    "listdir = os.listdir(path)\n",
    "\n",
    "plt.imshow(cv2.imread(train_images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(mask - pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(type(train_dataset.take(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train_dataset.shuffle(false)\n",
    "\n",
    "for elem in train_dataset.take(2):\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(elem[0][0])\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(elem[1][0])\n",
    "\n",
    "    print(elem[0].shape)\n",
    "    pred = model.predict(elem[0])\n",
    "    print('pred ',pred.shape)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(pred[0])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tensorflow': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "242939b0b6b0003f19ccbaf13e72bf1bb3ec31db06f26856a2ba8219e0fce7cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}